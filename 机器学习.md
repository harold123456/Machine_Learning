# 机器学习笔记
[TOC]
## 数据预处理
### 离散型特征
- One-Hot Encoding
  - 使用one-hot编码，将离散特征的取值扩展到了欧式空间，离散特征的某个取值就对应欧式空间的某个点。
  - 特征之间距离的计算或相似度的计算是非常重要的，而我们常用的距离或相似度的计算都是在欧式空间的相似度计算
  - 将离散型特征使用one-hot编码，确实会让特征之间的距离计算更加合理

### 连续性特征
- 先分组再进行离散化处理
- 标准化（归一化是标准化的一种，将数据统一映射到[0,1]区间上）
  - 好处
    1. 提升模型的收敛速度
    2. 提升模型的精度(大量纲的影响大)
    3. 深度学习中数据归一化可以防止模型梯度爆炸
  - 需要归一化的模型(LogisticReg，SVM，NeuralNetwork，SGD，决策树不需要)
    1. 有些模型在各个维度进行不均匀伸缩后，最优解与原来不等价，例如SVM（距离分界面远的也拉近）
    2. 有些模型在各个维度进行不均匀伸缩后，最优解与原来等价，例如logistic regression（因为θ的大小本来就自学习出不同的feature的重要性）由于实际求解往往使用迭代算法，如果目标函数的形状太“扁”，迭代算法可能收敛得很慢甚至不收敛（模型结果不精确）
  - 方法
    1. min-max标准化
    ```math
    x^* = (x - min) / (max - min)
    ```
    2. z-score标准化
    ```math
    x^* = (x - \mu) / \sigma
    ```
    3. log函数变换
    ```math
    x^* = log_{10}(x) / log_{10}(max)
    ```
    4. Sigmoid/Softmax变换
    5. L2范数归一化
      - 特征向量中每个元素均除以向量的L2范数


## 正则化

### 减轻过拟合
- 减少特征个数
- 正则化

## SVM
### 由逻辑回归损失函数得到SVM损失函数

- 调整logit回归的损失函数
![Snipaste_2019-04-02_11-33-11](51627463871C4E50A46BAC70F94301F4)
![2](487E6CCE2BF2431AAA85DFEC24895802)

- 损失函数
![3](74F612C199784A52835D54C62F6CFC64)
- 核函数及如何选定l1，l2...
![4](A3B66A0FF191440483722B0633545551)
![5](B0DFB67E755E4A9BA2E90F26595F8B0A)

### 使用SVM
- 选择参数C和kernel，若使用高斯核还需选择σ
- 在使用高斯核前，要对数据归一化
- 如何选SVM和LR  (n表示维度，m表示样本数)
  - n > m (n=10000,m=10-1000)，使用LR或线性SVM
  - n小m中等(n=1-1000,m=10-10000)，高斯核
  - n小m大(n=1-1000,m=50000+)，创建更多特征，使用LR或线性SVM
- SVM属于凸优化问题，不必过多担心局部最小

## 应用机器学习的建议

### 模型选择
- 训练集训练模型，验证集选择模型，测试集评估模型

### 偏差与方差
- 偏差![Snipaste_2019-03-31_14-51-12](BA36BAF8778A4171A0EB82E1E88E5753)
- 方差![Snipaste_2019-03-31_14-52-00](92C1008D83BB4CC99B7904FA27FC241C)
- 偏差和方差随正则化参数的变化![Snipaste_2019-03-29_01-11-17](0CEC15CDDF604DF99E5E5624384734D5)


## 无监督学习 k-means聚类
### 算法
![Snipaste_2019-03-31_11-13-46](7A8039BDF7DD44E2B9273FA4C4D1A5B7)
### 损失函数
![Snipaste_2019-03-31_11-11-30](3CA2292A7F5B439AA3AEF399BDF0627C)
### 随机初始化(K比较小 2-10，效果显著)
- 初始化可能会落入局部最优![Snipaste_2019-03-31_11-17-36](4E6055848A9C49A98B464731C1ADBEA6)
- 解决办法(多次随机初始化)![Snipaste_2019-03-31_11-37-01](934A39EF134A412FB004341BB124203E)

### 选择聚类数
  - “肘部法则”(拐点)![Snipaste_2019-03-31_11-40-28](58D962B3183F441AA5884FAA7FBBC9EF)
  - 根据后续目的选择

## 大规模机器学习

### 随机梯度下降(1个数据)、mini—batch梯度下降(b个数据)

### 在线学习(数据流)
- 对传入的数据更新并舍弃

### MapReduce
![Snipaste_2019-03-31_14-57-09](43803D45AA80448AA0C2ED254FDB0774)

### 上限分析
  - 衡量各个模块的提升对整体系统的提升程度