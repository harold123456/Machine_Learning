
[TOC]

## GBDT(Gradient Boosting Decision Tree)
### DT(回归树)
1. 求回归树
  - 选择最优切分变量j和最优切分点s
  - 用选定的对(j,s)划分区域并决定相应的输出值
  - 继续对子区域调用上面两步骤，直至满足终止条件
  - 将输入空间划分为M个区域，每个区域单元上有一个固定输出

### BDT
> 采用加法模型和前向分布算法，以决策树为基函数
- 回归问题的提升树算法
![Snipaste_2019-04-17_01-41-01](C405884FD06841C4930A76D7D9C56D52)
- 注：因为GBDT模型考虑的变量变少，减小过拟合

### GBDT
> 当损失函数每一步的优化不再容易，使用损失函数的负梯度在当前模型的值作为回归问题提升树算法中的残差的近似值，拟合一个回归树
- 梯度提升算法
![Snipaste_2019-04-17_01-55-59](72A10A828C924658A4006D1F3B19FB65)

- 用于分类问题![Snipaste_2019-04-17_14-42-03](52B8733CFE9F45A7BDDACC3D9806455B)
### GBDT和RF区别
1. GBDT是boosting方法，RF是bagging方法
2. GBDT核心是使用模型(CART、RF)拟合损失函数梯度，即期望输出与预测值的差，bias；RF核心是自采样(样本随机)和特征随机(所选子样本的最优特征来划分)，数据扰动导致模型学习性能的变化，即variance

### GBDT为什么用负梯度代替残差
![00ba6b170ccc09313116beb8fbf94ca](A5C5A6FAC98D451A8FFB9B82813640B1)

## XGBoost
> 缓解当数据复杂时，boosting可能迭代次数过多的问题
### 推导

1. XGBoost目标函数与泰勒展开
> GBDT使用一阶导，XGBoost使用二阶导

![Snipaste_2019-04-17_02-10-24](673E02B04D634307A1CB8205C57677A4)

2. 决策树的复杂度
![Snipaste_2019-04-17_02-16-25](097C88E0034D4EA18095063D470117DC)
![Snipaste_2019-04-17_02-14-33](E25B1EC278C045928006E374B2905354)

3. 目标函数的最小化
![Snipaste_2019-04-17_02-23-54](7A46B359AD9D42FF966A0C080F09CF9E)

4. 如何最优化目标函数（贪心算法）
  - 从深度为0的树开始，对每个叶节点枚举所有的可用特征
  - 针对每个特征，把属于该节点的训练样本根据该特征值升序排列，通过线性扫描的方式来决定该特征的最佳分裂点，并记录该特征的最大收益（采用最佳分裂点时的收益）
  - 选择收益最大的特征作为分裂特征，用该特征的最佳分裂点作为分裂位置，把该节点生长出左右两个新的叶节点，并为每个新节点关联对应的样本集
  - 回到第1步，递归执行到满足特定条件为止
![Snipaste_2019-04-17_15-49-03](5998DFCC65CA4D4A8C2B3F28F1641A86)
5. xgboost(gbdt)算法总结![Snipaste_2019-04-17_15-58-29](007A484C0ACB4808AE9FAB108A9CFC4A)

